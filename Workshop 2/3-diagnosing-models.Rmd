---
title: "Introduction to Multi-level Models using R"
author: "Professor Di Cook, Econometrics and Business Statistics"
date: "Workshop for the Institute for Safety, Compensation and Recovery Research"
output:
  beamer_presentation: 
    theme: Monash
---

```{r setup, include = FALSE}
library("knitr")
opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  cache = FALSE,
  fig.height = 3,
  fig.width = 6,
  fig.caption = FALSE,
  collapse = TRUE,
  comment = "#"
)
options(digits=2)
library("rmarkdown")
library("devtools")
library("readr")
library("tidyr")
library("ggplot2")
library("ggthemes")
library("gridExtra")
library("dplyr")
library("lubridate")
library("GGally")
library("rworldmap")
library("ggmap")
library("scales")
library("dichromat")
library("RColorBrewer")
library("viridis")
library("purrr")
library("broom")
library("timeDate")
library("haven")
library("boot")
library("lme4")
library("nlme")
library("HLMdiag")
library("mlmRev")
library("nortest")
```

# Outline

- Session 1: Basic models, fitting multiple separate models
- Session 2: Putting it together, using mixed effects models
- Session 3: Summarising and visualising models
- Session 4: Advanced modeling

# Session 2

- **Summarising and visualising models**

# Assumptions

Recall:

 $$\underset{(n_i \times 1)}{{\bf y}_i} =  \underset{(n_i \times p)}{{\bf X}_i} \underset{(p \times 1)}{{\boldsymbol \beta}} + \underset{(n_i \times q)}{{\bf Z}_i} \underset{(q \times 1)}{{\bf b}_i} + \underset{(n_i \times 1)}{{\boldsymbol \varepsilon}_i}$$

- ${\bf b}_i$ is a random sample from $\mathcal{N}({\bf 0}, {\bf D})$ and independent from the level-1 error terms,  
- ${\boldsymbol \varepsilon}_i$ follow a $\mathcal{N}({\bf 0},\sigma^2 {\bf R}_i)$ distribution 
- ${\bf D}$ is a positive-definite $q \times q$ covariance matrix and ${\bf R}_i$ is a positive-definite $n_i \times n_i$ covariance matrix

# Extract and examine level-1 residuals

```{r echo=FALSE, results='hide'}
radon_keep <- radon %>% group_by(county) %>% 
  tally() %>% filter(n > 4)
radon_sub <- radon %>% 
  filter(county %in% radon_keep$county & log.radon > -2)
radon_sub$basement <- 
  factor(radon_sub$basement, levels=c(0,1), 
         labels=c("basement", "first floor"))
radon_lmer <- lmer(log.radon ~ basement + uranium + 
  (basement | county.name), data = radon_sub)
```

```{r fig.show='hide'}
radon_lmer_fit <- radon_sub 
radon_lmer_fit$fit <- fitted(radon_lmer)
radon_lmer_fit$resid1 <-  HLMresid(radon_lmer, 
           level=1)
ggplot(radon_lmer_fit, aes(x=resid1)) + 
  geom_histogram(binwidth=0.5) 
```

${\boldsymbol \varepsilon}_i \sim \mathcal{N}({\bf 0},\sigma^2 {\bf R}_i)$

# Your turn

For the radon data:

- What is $p$, $q$, $g$?
- And hence $n_i, i=1, \dots, g$?

$$log.radon_{ij} = \beta_0 + \beta_1 basement_{ij} + \beta_2 uranium_i + b_{0i} + b_{1i} basement_{ij} + \varepsilon_{ij}$$

$$~~~ i=1, ..., \#counties; j=1, ..., n_i$$

```{r eval=FALSE, echo=FALSE}
# p=2 (basement, uranium)
# q=1 (basement)
length(unique(radon_sub$county.name)) # g
radon_sub %>% group_by(county.name) %>% tally()
```

# 

```{r echo=FALSE}
ggplot(radon_lmer_fit, aes(x=resid1)) + 
  geom_histogram(binwidth=0.5) 
```

Level-1 (observation level) look normal. 

# Normal probability plot

```{r fig.show='hide'}
ggplot_qqnorm(radon_lmer_fit$resid1, line="rlm") +
  theme(aspect.ratio=1)
```

#

```{r fig.width=2.8}
ggplot_qqnorm(radon_lmer_fit$resid1, line="rlm") +
  theme(aspect.ratio=1)
```

Level-1 (observation level) do nearly normal. 

# Examine within group

Summary statistics

```{r}
radon_lmer_fit %>% group_by(county.name) %>%
  summarise(m = mean(resid1), s = sd(resid1), n = length(resid1)) %>%
  head()
```

#

```{r echo=FALSE}
res.sum <- radon_lmer_fit %>% group_by(county.name) %>%
  summarise(m = mean(resid1), s = sd(resid1), n = length(resid1))
ord <- order(res.sum$m)
radon_lmer_fit$county.name <- factor(radon_lmer_fit$county.name, levels=res.sum$county.name[ord])
```

```{r fig.width=3.5}
ggplot(radon_lmer_fit, aes(x=county.name, y=resid1)) + 
  geom_point(alpha=0.5) + coord_flip()
```

# Normality tests

Anderson-Darling, Cramer-von Mises, Lilliefors (Kolmogorov-Smirnov)

```{r results='hide'}
library("nortest")
ad.test(radon_lmer_fit$resid1)
cvm.test(radon_lmer_fit$resid1)
lillie.test(radon_lmer_fit$resid1)
```

```{r echo=FALSE}
ad.test(radon_lmer_fit$resid1)
```

all believe that the residuals are consistent with normality.

# Conclusion about level-1 residuals

The assumption: 

$${\boldsymbol \varepsilon}_i \sim \mathcal{N}({\bf 0},\sigma^2 {\bf R}_i)$$

is probably ok, at the worst it is not badly violated. 

# Random effects

$${\bf b}_i \sim \mathcal{N}({\bf 0}, {\bf D}), ~~~ i=1, \dots g$$

where ${\bf D}$ allows for correlation between random effects within group, and these should be independent from the level-1 error

# Level 2 (random effects)

```{r}
rf <- HLMresid(radon_lmer, level="county.name") 
# same as ranef(radon_lmer)
rf$county.name <- rownames(rf)
rf <- rf %>% rename(resid.basement=`(Intercept)`, 
                    resid.ff=`basementfirst floor`)
radon_lmer_fit <- merge(radon_lmer_fit, rf, 
                        by="county.name")
```

We have both intercepts (basement) and slopes (first floor)

#

```{r fig.width=2.2, fig.show='hold'}
ggplot(radon_lmer_fit, aes(x=resid.basement)) + 
  geom_histogram(binwidth=0.05) 
ggplot(radon_lmer_fit, aes(x=resid.ff)) + 
  geom_histogram(binwidth=0.2) 
```

#

```{r fig.width=2.2, fig.show='hold'}
ggplot_qqnorm(radon_lmer_fit$resid.basement, line="rlm") +
  theme(aspect.ratio=1)
ggplot_qqnorm(radon_lmer_fit$resid.ff, line="rlm") +
  theme(aspect.ratio=1)
```

# Should be no correlation 

```{r}
ggplot(radon_lmer_fit, aes(x=resid.basement, y=resid.ff)) + 
  geom_point() + theme(aspect.ratio=1) 
```

# Fitted vs Observed

Plotting the observed vs fitted values, gives a sense for how much of the response is explained by the model. Here we can see that there is still a lot of unexplained variation.

```{r fig.show='hide'}
ggplot(radon_lmer_fit, aes(x=fit, y=log.radon)) + 
  geom_point()  
```

#

```{r echo=FALSE}
ggplot(radon_lmer_fit, aes(x=fit, y=log.radon)) + 
  geom_point()  
```

# Autism model

```{r echo=FALSE}
autism_keep <- autism %>% group_by(childid) %>% 
  tally(sort=TRUE) %>% filter(n>2)
autism_sub <- autism %>% 
  filter(childid %in% autism_keep$childid)
```

```{r results='hide'}
autism_lmer4 <- lmer(vsae ~ age2*sicdegp + 
                       age2*bestest2 + age2*race + 
                       ( age2 - 1 | childid ), 
                    data = autism_sub)
autism_lmer_fit <- augment(autism_lmer4)
```

# Your turn

- What is $p$, $q$, $g$?
- And hence $n_i, i=1, \dots, g$?

# Your turn

Write down the model statement that corresponds to the R code fit:

```
autism_lmer4 <- lmer(vsae ~ age2*sicdegp + 
                       age2*bestest2 + age2*race + 
                       ( age2 - 1 | childid ), 
                    data = autism_sub)
```

# Your turn

What does the function `augment` do? (Hint: it is in the `broom` package.)

# Plot the model: random effects

```{r fig.show='hide'}
ggplot(autism_lmer_fit, aes(x=age2, y=vsae)) + 
  geom_point(alpha=0.2) +
  geom_line(aes(y=.fitted, group=childid, color=sicdegp)) + 
  facet_grid(race~bestest2)
```

#

```{r echo=FALSE}
ggplot(autism_lmer_fit, aes(x=age2, y=vsae)) + 
  geom_point(alpha=0.2) +
  geom_line(aes(y=.fitted, group=childid, color=sicdegp)) + 
  facet_grid(race~bestest2)
```

# Fixed effects

```{r fig.show='hide'}
ggplot(autism_lmer_fit, aes(x=age2, y=vsae)) + 
  geom_point(alpha=0.2) +
  geom_line(aes(y=.fixed, color=sicdegp)) + 
  facet_grid(race~bestest2)
```

#

```{r echo=FALSE}
ggplot(autism_lmer_fit, aes(x=age2, y=vsae)) + 
  geom_point(alpha=0.2) +
  geom_line(aes(y=.fixed, color=sicdegp)) + 
  facet_grid(race~bestest2)
```

# Fixed effects

```{r fig.show='hide'}
ggplot(autism_lmer_fit, aes(x=age2, y=vsae)) + 
  geom_point(alpha=0.2) +
  geom_line(aes(y=.fixed, color=interaction(race,bestest2), linetype=sicdegp)) 
```

#

```{r echo=FALSE}
ggplot(autism_lmer_fit, aes(x=age2, y=vsae)) + 
  geom_point(alpha=0.2) +
  geom_line(aes(y=.fixed, color=interaction(race,bestest2), linetype=sicdegp)) 
```

# Your turn

- Compute the level-1 residuals
- Conduct a Lilliefors test of normality

```{r echo=FALSE, eval=FALSE}
autism_lmer_fit$resid1 <- HLMresid(autism_lmer4, level=1)
ggplot(autism_lmer_fit, aes(x=resid1)) + 
  geom_histogram(binwidth=2.5) 
```

- Do the residuals look normal?

# Your turn

- Plot the observed vs fitted data
- Does the model explain a substantial amount of the variation?

# Diagnostics of influence, outlier detection

- Leave-one-out statistics form the basis of diagnostics. Examine the change in the model estimates with and without the case. 
- For multilevel models, there are multiple levels of removal. Could be one of the group level structures, or individuals within groups
- The `HMLdiag` package makes it easy to compute and examine these, e.g. Cooks distance, mdffits, leverage

# Exam data from mlmRev package

```{r}
library("mlmRev")
glimpse(Exam)
```

# Fit model

```{r results='hide'}
fm4 <- lmer(normexam ~ standLRT + I(standLRT^2) + 
              I(standLRT^3) + sex + schgend + schavg + 
              (standLRT | school), data = Exam, REML = FALSE)
fm4
```

# Fixed effects

```
Fixed effects:
               Estimate Std. Error t value
(Intercept)   -0.019994   0.049379  -0.405
standLRT       0.594155   0.025732  23.090
I(standLRT^2)  0.013283   0.009004   1.475
I(standLRT^3) -0.014178   0.005503  -2.577
sexM          -0.170294   0.033830  -5.034
schgendboys    0.186847   0.094959   1.968
schgendgirls   0.164680   0.075212   2.190
schavg         0.287547   0.098420   2.922
```

# Random effects

```
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 school   (Intercept) 0.06507  0.2551       
          standLRT    0.01419  0.1191   0.48
 Residual             0.54904  0.7410       
Number of obs: 4059, groups:  school, 65
```

# Model summary

```
     AIC      BIC   logLik deviance df.resid 
  9289.3   9365.0  -4632.6   9265.3     4047 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8524 -0.6301  0.0238  0.6853  3.4516 
```

# Cooks distance

Measures difference in standardised residual, based on prediction with model computed with and without the unit

```{r fig.show='hide'}
library("HLMdiag")
cooksd_fm4  <- cooks.distance(fm4, group = "school")
dotplot_diag(x = cooksd_fm4, cutoff = "internal",
   name = "cooks.distance") + ylab("Cook s distance") + xlab("school")
```

#

```{r echo=FALSE}
dotplot_diag(x = cooksd_fm4, cutoff = "internal",
   name = "cooks.distance") + ylab("Cook s distance") + 
  xlab("school")
```

This would indicate that school 25 is an anomaly. 

# mdffits

The statistic *dffits* measures the difference in the fitted value, with and without the unit.

```{r out.width=7}
mdffits_fm4 <- mdffits(fm4, group = "school")
sort(mdffits_fm4)
```

# Influence

```{r}
leverage_fm4 <- leverage(fm4, level = "school")
head(leverage_fm4)
```

# Reference material

- Loy and Hofmann (2015) "Are You Normal? The Problem of Confounded Residual Structures in Hierarchical Linear Models", Journal of Computational and Graphical Statistics
- Loy and Hofmann (2014) HLMdiag: A Suite of Diagnostics for Hierarchical Linear Models in R, Journal of Statistical Software

# Credits

Notes prepared by Di Cook, using material developed by Hadley Wickham, Heike Hofmann and Adam Loy.
